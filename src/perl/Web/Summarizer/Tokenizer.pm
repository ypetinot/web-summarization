=pod
package Web::Summarizer::Tokenizer::Heap::EdgeElem;

use strict;
use warnings;

use Moose;
use MooseX::NonMoose::InsideOut;
use namespace::autoclean;

extends 'Heap::Elem';

has 'from'   => ( is => 'ro' , isa => 'Str' , required => 1 );
has 'to'     => ( is => 'ro' , isa => 'Str' , required => 1 );
has 'weight' => ( traits => qw/Counter/ , is => 'rw' , isa => 'Num' , default => 0 );

# TODO : is there a way to achieve this using a trait ?
sub cmp {
    my $self = shift;
    my $other = shift;

    $self->weight cmp $other->weight;
}

1;
=cut

package Web::Summarizer::Tokenizer;

# TODO : find better class name ?
# Tokenizer that analyzes the full content of a Web object to determine how to optimally segment each text utterance.

# **************************************************************************************************************************************************************************************************************
# Future work (alternative approaches to segmentation)

# 2 - Metropolis Hastings exploration of segmentation space (or is it better to go straight into testing Simulated Annealing ?)
# C++ implementation
 
# ILP formulation ? => binary variable for every segmentable point: specific points in URL, all single space / punctuation locations for rest of content
#                   => minimize number of token types ==> problem is how to handle identities ... => no functional formulation
#                   => not possible ...

# MH algorithm => c++ => random status for segmentation points (the state of the system) [ok]
#                     => energy of the system is number of word types => single scan of data at each iteration (might be updateable in an iterative way actually) [ok]
#                     => state transitions => all transitions equally likely but should be for all associated points => easy to index utterances where the token appears and that would need to be reprocessed
#                     => * New * York * City *  ==> * New York * City

# multinomial => same principle => prior distribution controling the position of segmentation points
#             => each utterance is generated by 
# **************************************************************************************************************************************************************************************************************

no Carp::Assert;
use Graph::Directed;
use Heap::Elem::Ref( RefElem );
use Heap::Fibonacci;
use List::MoreUtils qw/uniq/;

use Moose;
use namespace::autoclean;

extends 'String::Tokenizer';

with( 'Logger' );

# associated object => tokenizer is optimized for the target object
has 'object' => ( is => 'ro' , isa => 'Category::UrlData' , required => 1 );

has '_basic_tokenizer' => ( is => 'ro' , isa => 'String::Tokenizer' , init_arg => undef , lazy => 1 , builder => '_basic_tokenizer_builder' );
sub _basic_tokenizer_builder {
    return new String::Tokenizer;
}

# TODO : serialize multi-token phrase (as an indication of pre-processing) => problem how do we ensure consistency with raw content ?
has '_raw_tokens' => ( is => 'ro' , isa => 'ArrayRef' , init_arg => undef , lazy => 1 , builder => '_raw_tokens_builder' );
sub _raw_tokens_builder {

    # heuristics ? graph with unique tokens => increment edge weight for each cooccurrence => iteratively remove lowest cost edge and collect trees
    # => sampling ? i.e. generate graph, remove lowest cost edge randomly and plot distribution of disconnected components => may even yield variations on the same term
    # => problem, does this process support URL strings ? => longest lcs between url string and any sentence in the 
    
    # by definition host is specific ? => lcs for tokenization and removal ?
    # => treat domain as a primer (for specificity) ?
    # => is the domain name always specific ? the domain has to be specific is only for ownership purposes
    # => what about the path portion ?
    # => what about the host(name) portion ?
    # very specific is fine, problem is regular word that 

    # Possible solution (is there a more efficient one) ?
    # Start abstracting using domain name (essentially owning entity) by performing LCS between URL (domain name) and all utterances => contiguous phrases that appear more than once (can stop early) can be treated as phrases (if specific or always ?) => maybe compare against unigram probabilities ?

    # **************************************************************************************************************************************************
    # TO BE ADDED TO PAPER
    # Might be worth having a discussion as to whether or not focusing on Web Site Summarization wouldn't be easier (at least present data for root URLs vs deep URLs) ?
    # anonymize content (modalities) based on URL
    # problem => how far do we go ? how "Generic/Specific" is a URL term ? => it may carry no direct semantic meaning e.g. Google or Tall Timber ...
    # basic problem: publications vs kathy ...
    # Solution #1 - simple => do not use URL to generate the signature
    # Solution #2 - treat domain name as named entity + any other term in URL that is detected as named entity (how do we do that without relying on capitalization ?)
    # technically the problem should only exist if specific term (or phrase) appears in a short string ... maybe we should ignore the title as well then
    
    # The real source of short/fragmented content is the page itself (excluding the title since it is in general short and thus ambiguous) but can be boosted by all other modalities => a string that is short in the content has a navigational value => functionality
    # TODO : anonmyze portions of content that is guaranteed to be specific => add discussion on this to paper
    # TODO : verify simple example .../publications.html to make sure publications gets the highest level of boost

    # TODO (only way to succeed) / Presentation : (1) anonymize / (2) rank by decreasing genericity / (3) minimum guarantees for support/relevance (use signature ?) => default to other ? / (4) adapt
    # ***************************************************************************************************************************************************

    # CURRENT : implementing graph-based segmentation => graph is constructed from the content of fluent modalities only
    # NEXT : align url components to segmented phrases
    # NEXT : what phrases do we anonymize ?

    my $this = shift;

    my $tokenization_graph = new Graph::Directed;
    ###my $tokenization_graph_edge_heap = Heap::Fibonacci->new;
    
    # 1 - list of basic tokens, with segment for each token (=> we assume european tokenization)
    my @fluent_segments;
    push @fluent_segments , @{ $this->object->title_modality->segments };
    push @fluent_segments , @{ $this->object->content_modality->segments };
    my %token_2_fluent_segment;

    # 2 - build graph
    map {

	my $fluent_segment_tokens = $this->_basic_tokenizer->tokenize( $_ , normalize_case => 1 );
	my $fluent_segment_length = scalar( @{ $fluent_segment_tokens } );
	
	for ( my $i = 0 ; $i < ( $fluent_segment_length - 1 ) ; $i++ ) {
	   
	    my $from_token = $fluent_segment_tokens->[ $i ];
	    my $to_token = $fluent_segment_tokens->[ $i + 1 ];

	    my $current_edge_weight;

# might not be necessary
=pod
	    # check for cycle => we don't add edges that result in cycles => multiple random initialization needed to find optimal segmentation ?
	    if ( $tokenization_graph->is_reachable( $from_token , $to_token ) ) {
		print STDERR ">> dropping direct edge between [$from_token] and [$to_token] ...\n";
		next;
	    }
=cut

	    if ( ! $tokenization_graph->has_edge( $from_token , $to_token ) ) {
		$tokenization_graph->add_edge( $from_token , $to_token );
		$current_edge_weight = 0;
	    }
	    else {
		$current_edge_weight = $tokenization_graph->get_edge_weight( $from_token , $to_token );
	    }

	    my $updated_edge_weight = $current_edge_weight + 1;
	    $tokenization_graph->set_edge_weight( $from_token , $to_token , $updated_edge_weight );

	}


    } @fluent_segments;

=pod
    # 3 - build heap
    # Note : a sorted array would do actually since we are going to process all edges anyways ...
    my @graph_edges = $tokenization_graph->edges;
    foreach my $graph_edge (@graph_edges) {
	my $edge_heap_object = new Web::Summarizer::Tokenizer::Heap::EdgeElem( from => $from_token , to => $to_token );
	my $edge_heap_elem = RefElem( $edge_heap_object );
	$tokenization_graph_edge_heap->add( $edge_heap_elem );
    }
=cut

    # process graph
    my $raw_tokens = $this->_deconstruct_graph( $tokenization_graph , 2 );

    my $url = $this->object->url;
    map {
	$this->info( "Identified token for ($url): $_" );
    } @{ $raw_tokens };

    return $raw_tokens;

}

sub _add_component {
    
    my $this = shift;
    my $tokens = shift;
    my $component_edges = shift;

    # add as token all contiguous sequences that can be derived from the set of component edges, starting with the longest sequences
    
    # TODO : could we do better ?
    my $component_graph = new Graph::Directed;
    map { $component_graph->set_edge_weight( @{ $_ } ); } @{ $component_edges };

    # generate all possible paths (all pairs shortest paths) in component and filter by actual occurrence in raw data
    # Note : implicitly this will only generate paths of length > 1 , which means we will not look at single tokens
    my $apsp = $component_graph->all_pairs_shortest_paths;
    my @all_pairs_shortest_paths;
    my @component_nodes = $component_graph->vertices;
    for ( my $i=0; $i<=$#component_nodes; $i++) {
	for ( my $j=0; $i<=$#component_nodes; $i++) {
	    if ( $i == $j ) {
		next;
	    }
	    my @shortest_path_ij = $apsp->path_vertices( $component_nodes[ $i ] , $component_nodes[ $j ] );
	    if ( $#shortest_path_ij > -1 ) {
		push @all_pairs_shortest_paths , \@shortest_path_ij;
	    }
	}
    }
    my @all_pairs_shortest_paths_sorted = sort { scalar( @{ $b } ) <=> scalar( @{ $a } ) } @all_pairs_shortest_paths;
    foreach my $shortest_path (@all_pairs_shortest_paths) {
	
	my $path_length = scalar( @{ $shortest_path } );
	my $path_regex = join( '\s*' , map { qr/\Q$_\E/i } @{ $shortest_path } );

	# Note : "dynamic" threshold that is equal to length => the longer a phrase, the more support we want
	if ( $this->object->supports( $path_regex , raw => 1 ) >= $path_length ) {
	    #print STDERR ">> Detected token : $path_regex\n";
	    push @{ $tokens } , [ $path_regex , $path_length ];
	}

    }
    
}

sub _edge_key {
    my $this = shift;
    return join( " --- " , sort { $a cmp $b } @_ );
}

sub _deconstruct_graph {

    my $this = shift;
    my $directed_graph = shift;
    my $threshold = shift;

    $this->logger->debug( "identifying frequent phrase using phrase graph deconstruction ..." );

    my @tokens;

    my @weighted_edges = map { [ @{ $_ } , $directed_graph->get_edge_weight( @{ $_ } ) ] } $directed_graph->edges;

    # TODO : is threshold arbitrary ? => dynamic threshold ? => max( $threshold , max freq - 1 / 2 ) ?
    my @selected_edges = grep { $_->[ 2 ] >= $threshold } @weighted_edges;
    my @selected_vertices = uniq map { ( $_->[ 0 ] , $_->[ 1 ] ) } @selected_edges;

    my @sorted_edges = sort { $a->[ 2 ] <=> $b->[ 2 ] } @selected_edges;

    my @connected_components = ( \@selected_vertices );

    my %edge2seen;
    while ( $#connected_components >= 0 ) {

	$this->logger->trace( "processing new connected component / $#connected_components" );

	my $connected_component = shift @connected_components;

	# component vertices
	my %component_vertices;
	map { $component_vertices{ $_ } = 1; } @{ $connected_component };

	# component edges
	my @component_edges_sorted = grep {
	    defined( $component_vertices{ $_->[ 0 ] } ) && defined( $component_vertices{ $_->[ 1 ] } ) && ( !defined( $edge2seen{ $this->_edge_key( $_->[ 0 ] , $_->[ 1 ] ) } ) )
	} @sorted_edges;

	affirm { ! scalar( grep { defined( $edge2seen{ $this->_edge_key( $_->[ 0 ] , $_->[ 1 ] ) } ) } @component_edges_sorted ) } "undirected graph contains previously seen edges" if DEBUG;

	# determine edge weight distribution for this component
	my %weight2count;
	map {

	    my $edge_weight = $_->[ 2 ];
	    $weight2count{ $edge_weight }++;

	} @component_edges_sorted;

	# if all edges are of the same weight there's nothing else we can do
	if ( scalar( keys( %weight2count ) ) == 1 ) {
	    $this->_add_component( \@tokens , \@component_edges_sorted );
	    next;
	}

	# generate undirected graph for component
	my $undirected_graph = new Graph::Undirected;
	map {
	    $undirected_graph->set_edge_weight( @{ $_ } );
	} @component_edges_sorted;

	# iteratiely delete edges from the current component until we create a new component
	foreach my $sorted_weighted_edge (@component_edges_sorted) {
	    
	    my @current_edge = ( $sorted_weighted_edge->[ 0 ] , $sorted_weighted_edge->[ 1 ] );
	    my $edge_weight = $sorted_weighted_edge->[ 2 ];
	    
	    $this->logger->trace( "deleting edge " . join( " => " , @current_edge ) . " / $edge_weight / " . scalar( @component_edges_sorted ) );

	    # delete edge
	    # Note : this removes the edges, but not the nodes
	    affirm { $undirected_graph->has_edge( @current_edge ) } "edge does not exist in graph" if DEBUG;
	    $undirected_graph->delete_edge( @current_edge );
	    affirm { ! $undirected_graph->has_edge( @current_edge ) } "edge still exists in graph" if DEBUG;
	    $edge2seen{ $this->_edge_key( @current_edge ) }++;
	    
	    # check if removing edge created a disconnected component
	    #           => if we create a new connected component we know we have a "phrase cluster" (or at least we assume so)
	    #           => get components in undirected graph and then later on only validate sequences for which there exists a path in the directed graph
	    if ( ! $undirected_graph->is_connected ) {
		# we have identified a new cluster

		# TODO : remove code duplication
		# Note : we must filter out single-node components => these will be handled as regular tokens
		push @connected_components , grep { scalar( @{ $_ } ) > 1 } $undirected_graph->connected_components;

		# TODO : is this the right thing to do at this point ?
		last;
	    }
	    
	}
	
    }

    my @sorted_tokens = map { $_->[ 0 ] } sort { length( $b->[ 0 ] ) <=> length( $a->[ 0 ] ) } sort { $b->[ 1 ] <=> $a->[ 1 ] } @tokens;
    return \@sorted_tokens;

}

has '_object_tokens_regex' => ( is => 'ro' , isa => 'Regexp' , init_arg => undef , lazy => 1 , builder => '_object_tokens_regex_builder' );
sub _object_tokens_regex_builder {
    my $this = shift;
    my $regex_string = join ( "|" , map { "(?:$_)" } @{ $this->_raw_tokens } );
    return qr/$regex_string/;
}

=pod
# actual tokenization
sub _tokenize {

    my $this = shift;
    my $text = shift;

    my @tokens;

    my $object_tokens_regex = $this->_object_tokens_regex;
    while ( $text =~ m/(.*)($object_tokens_regex)(.*)/sg ) {

	my $pre_object_token = $1;
	my $object_token = $2;
	my $post_object_token = $3;

	# TODO : can we make the following sequence look a little bit nicer ?

	if ( length( $pre_object_token ) ) {
	    push @tokens , @{ $this->_basic_tokenizer->tokenize( $pre_object_token ) } ,
	}

	push @tokens , $object_token;

	if ( length( $post_object_token ) ) {
	    push @tokens , @{ $this->_basic_tokenizer->tokenize( $post_object_token ) };
	}

    }

    return \@tokens;

}
=cut

# actual tokenization
sub _tokenize {

    my $this = shift;
    my $text = shift;

    my @tokens;

    my $regexes = $this->_raw_tokens;
    foreach my $regex (@{ $regexes }) {

	if ( $text =~ m/(.*)($regex)(.*)/sg ) {

	    my $pre_object_token = $1;
	    my $object_token = $2;
	    my $post_object_token = $3;

	    # TODO : can we make the following sequence look a little bit nicer ?
	    
	    if ( length( $pre_object_token ) ) {
		push @tokens , @{ $this->_tokenize( $pre_object_token ) };
	    }
	    
	    push @tokens , $object_token;
	    
	    if ( length( $post_object_token ) ) {
		push @tokens , @{ $this->_tokenize( $post_object_token ) };
	    }
	    
	    return \@tokens;

	}

    }
	
    return $this->_basic_tokenizer->tokenize( $text );

}

sub _tokenize_recursive {

    my $this = shift;
    my $queue = shift;
    my $regexes = shift;
    
    my @tokens;
    my @processing_queue;
    
    while ( scalar( @{ $queue } ) ) {

	my $queue_entry = shift @{ $queue };

	while ( scalar( @{ $regexes } ) ) {

	    my $token_regex = shift @{ $regexes };
	    my @entry_segments = split /$token_regex/ , $queue_entry;

	    while ( $#entry_segments >= 0 ) {
		push @processing_queue , ( shift @entry_segments );
		if ( $#entry_segments >= 0 ) {
		    #push @
		}
	    }
	}
	
    }

    return \@tokens;

}

__PACKAGE__->meta->make_immutable;

1;
